# Hand-Gesture-Classification
Classifying hand gestures using landmark data generated by MediaPipe from the HaGRID (Hand Gesture Recognition Image Dataset)


- The input to the project is a CSV file containing hand landmarks (e.g., x, y, z coordinates of keypoints) extracted from the HaGRID dataset using MediaPipe. The output is a trained machine learning model capable of classifying hand gestures into predefined classes.

- The HaGRID dataset contains 18 classes of hand gestures, including:
        Call, Dislike, Fist, Four, Like, Mute, Ok, One, Palm, Peace, Peace Inverted, Rock, Stop, Stop Inverted, Three, Three2, Two Up, Two Up Inverted

- Each gesture is represented by a set of hand landmarks (21 landmarks per hand) extracted using MediaPipe. The CSV file contain landmarks(x,y,z location) along with their corresponding gesture labels.

- This repository has ipynb file and pkl file which is the best model reached (XGB Model) of accuracy almost 98%

- Input and Output Videos: https://drive.google.com/drive/folders/1UsDdhcEx9QeF5owZM-mxoyFodZ0oHIFR?usp=sharing
  
